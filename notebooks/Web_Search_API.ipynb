{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JboB5VaCJUrb",
        "outputId": "b01ee10d-82e2-41a5-8e34-6ab6a84cd075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.12 openai==1.59.6 tiktoken==0.8.0 llama-index-tools-google==0.3.0 newspaper4k==0.9.3.1 lxml_html_clean==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1NKAn5scN_g9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "GOOGLE_SEARCH_KEY = \"GOOGLE_SEARCH_KEY\"\n",
        "GOOGLE_SEARCH_ENGINE = \"GOOGLE_SEARCH_ENGINE\" # Search Engine ID\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY1')\n",
        "# GOOGLE_SEARCH_KEY = userdata.get('GOOGLE_SEARCH_KEY')\n",
        "# GOOGLE_SEARCH_ENGINE = userdata.get('GOOGLE_SEARCH_ENGINE')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM and Embedding Model"
      ],
      "metadata": {
        "id": "Yp-tXudCc3Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature= 0)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "dYmz-uAIc2rb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1gQVHvITMI"
      },
      "source": [
        "# Using Agents/Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "\n",
        "# define sample Tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "# initialize ReAct agent\n",
        "agent = ReActAgent.from_tools([multiply_tool], verbose=True)"
      ],
      "metadata": {
        "id": "OHd8WbUoZHWi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = agent.chat(\"What is the multiplication of 43 and 45?\")\n",
        "\n",
        "# Final response\n",
        "res.response"
      ],
      "metadata": {
        "id": "9JGVdS6AZGve",
        "outputId": "3f1274e1-7ab2-43ba-d4ec-65242a448dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 9ca7e45f-4345-4e35-bc75-e02b6e534492. Step input: What is the multiplication of 43 and 45?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: multiply\n",
            "Action Input: {'a': 43, 'b': 45}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 1935\n",
            "\u001b[0m> Running step 2f7b98b1-145d-4fb5-ad6c-30216d9d7125. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
            "Answer: The multiplication of 43 and 45 is 1935.\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The multiplication of 43 and 45 is 1935.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMypoqUyuXq"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4Q7sc69nJvWI"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VrbuIOaMeOIf"
      },
      "outputs": [],
      "source": [
        "# Import and initialize our tool spec\n",
        "from llama_index.core.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n",
        "\n",
        "# Wrap the google search tool to create an index on top of the returned Google search\n",
        "wrapped_tool = LoadAndSearchToolSpec.from_defaults(\n",
        "    tool_spec.to_tool_list()[0],\n",
        ").to_tool_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ENpLyBy7UL"
      },
      "source": [
        "## Create the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-_Ab47ppK8b2"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "\n",
        "agent = OpenAIAgent.from_tools(wrapped_tool, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YcUyz1-FlCQ8"
      },
      "outputs": [],
      "source": [
        "res = agent.chat(\"How many parameters LLaMA 3.2 model has?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w4wK5sY-lOOv",
        "outputId": "9429e77f-3603-43a8-d6a9-f55200997bbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The LLaMA 3.2 model has parameters in the following configurations: 1 billion, 3 billion, 11 billion, and 90 billion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM_cvBA1nTJM",
        "outputId": "57ab10be-9f3b-45d6-b66e-3fd785a6ddb4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ToolOutput(content='Content loaded! You can now search the information using read_google_search', tool_name='google_search', raw_input={'args': (), 'kwargs': {'query': 'LLaMA 3.2 model parameters'}}, raw_output='Content loaded! You can now search the information using read_google_search', is_error=False),\n",
              " ToolOutput(content='LLaMA 3.2 features models with parameters of 1 billion, 3 billion, 11 billion, and 90 billion.', tool_name='read_google_search', raw_input={'args': (), 'kwargs': {'query': 'LLaMA 3.2 model parameters'}}, raw_output='LLaMA 3.2 features models with parameters of 1 billion, 3 billion, 11 billion, and 90 billion.', is_error=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "res.sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "who-NM4pIhPn"
      },
      "source": [
        "# Using Tools w/ VectorStoreIndex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g9cTM9GI-19"
      },
      "source": [
        "A limitation of the current agent/tool in LlamaIndex is that it **relies solely on the page description from the retrieved pages** to answer questions. This approach will miss answers that are not visible in the page's description tag. To address this, a possible workaround is to fetch the page results, extract the page content using the newspaper3k library, and then create an index based on the downloaded content. Also, the previous method stacks all retrieved items from the search engine into a single document, making it **difficult to pinpoint the exact source** of the response. However, the following method will enable us to present the sources easily.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31G_fxxJIsbC"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lwRmj2odIHxt"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UVIxdj04Bsf2"
      },
      "outputs": [],
      "source": [
        "search_results = tool_spec.google_search(\"LLaMA 3.2 model details\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AlYDNfg2BsdQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "search_results = json.loads(search_results[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHALd3uhIxtQ"
      },
      "source": [
        "## Read Each URL Contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXz3JFduBsaq",
        "outputId": "41005455-975a-43a7-f7a9-023679c43296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:newspaper.network:get_html_status(): bad status code 403 on URL: https://www.reddit.com/r/LocalLLaMA/comments/1frwnpj/llama_32_vision_model_image_pixel_limitations/, html: <body class=theme-beta><div><style>.theme-light,:root{--rem360:22.5rem;--rem320:20rem;--rem192:12rem;--rem144:9rem;--rem128:8rem;--rem96:6rem;--rem90:5.625rem;--rem88:5.5rem;--rem64:4rem;--rem56:3.5re\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "for item in search_results[\"items\"]:\n",
        "\n",
        "    try:\n",
        "        article = newspaper.Article(item[\"link\"])\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if len(article.text) > 0:\n",
        "            pages_content.append(\n",
        "                {\"url\": item[\"link\"], \"text\": article.text, \"title\": item[\"title\"]}\n",
        "            )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(len(pages_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqxa_qRVI3G0"
      },
      "source": [
        "## Create the Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O4PkK8DuBsZT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "# Convert the texts to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(text=row[\"text\"], metadata={\"title\": row[\"title\"], \"url\": row[\"url\"]})\n",
        "    for row in pages_content\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2RtMBWpgBsWX"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Build index / generate embeddings using OpenAI.\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xV_ibEZ_BsM4"
      },
      "outputs": [],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nziwu27MI6ih"
      },
      "source": [
        "## Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xea7ZeidH27i",
        "outputId": "4a1abc15-f94f-424b-e40e-827df5894e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Llama 3.2 models have the following parameter sizes: 1B, 3B, 11B, and 90B.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"How many parameters LLaMA 3.2 model has? list exact sizes of the models\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QpGPD5nHORP",
        "outputId": "43d41489-0d66-423a-bb68-065d6bcb0ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title\t Introducing Llama 3.2 models from Meta in Amazon Bedrock: A new ...\n",
            "Source\t https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/\n",
            "Score\t 0.5448579358351563\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Title\t Introducing Llama 3.2 models from Meta in Amazon Bedrock: A new ...\n",
            "Source\t https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/\n",
            "Score\t 0.521867985403887\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in response.source_nodes:\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Source\\t\", src.metadata[\"url\"])\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B5b4nZ-qHpdP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}