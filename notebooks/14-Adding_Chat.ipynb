{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "QPJzr-I9XQ7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958b063b-68b6-4ff0-aced-0a4506d82895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.8/70.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.7/410.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.5/262.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.59.8 llama-index-finetuning==0.1.12 llama-index-embeddings-huggingface==0.2.3 llama-index-embeddings-cohere==0.1.9 llama-index-readers-web==0.1.23 cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 sentence-transformers==2.7.0 pydantic==2.10.0 llama-index-vector-stores-chroma==0.1.10 kaleido==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "60AA1iCrqApk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bbc63358f02540f6a5e476c4d02fd964",
            "48dcb706fd614fc8943e15d15a0b0fe2",
            "bc5a1a1acdbc4afb95c8e13ebed9b495",
            "41e89cbfd4804323989193fcb3dd7f7b",
            "16e1b25eb22a4e0a896afb9f85274ece",
            "03c5b98d4ea94568ad110f4e4bd402c6",
            "9fcb1f1b5a324d539bdf9f36df413d76",
            "5408c84b29a74670bd8d03f2d538926c",
            "dec4652142e442658c69d473abcc7b72",
            "2e16e13c258d4744b804cb1e0274f1c1",
            "86b5a4d6f70043d3a65de02b840e8937"
          ]
        },
        "outputId": "9a8a650e-650b-42b0-b7f2-40eab73a2091"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbc63358f02540f6a5e476c4d02fd964"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Downloading Vector store from Hugging face hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SodY2Xpf_kxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e779300-0ba3-4149-8962-d2a55e93708b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Load the vector store from the local storage.\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0m5rl195bcz"
      },
      "source": [
        "# Display result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4JpaHEmF5dSS"
      },
      "outputs": [],
      "source": [
        "# A simple function to show the response and the sources.\n",
        "def display_res(response):\n",
        "    print(\"Response:\\n\\t\", response.response.replace(\"\\n\", \"\"))\n",
        "\n",
        "    print(\"Sources:\")\n",
        "    if response.source_nodes:\n",
        "        for src in response.source_nodes:\n",
        "            print(\"\\tNode ID\\t\", src.node_id)\n",
        "            print(\"\\tText\\t\", src.text)\n",
        "            print(\"\\tScore\\t\", src.score)\n",
        "            print(\"\\t\" + \"-_\" * 20)\n",
        "    else:\n",
        "        print(\"\\tNo sources used!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbStjvUJ1cft"
      },
      "source": [
        "# Chat Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kwWlDpoR1cRI"
      },
      "outputs": [],
      "source": [
        "# define the chat_engine by using the index\n",
        "chat_engine = vector_index.as_chat_engine(llm=Settings.llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ER3Lb-oN46lJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44aff363-9d2e-41c1-f144-11c58f66842f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Parameter Efficient Fine Tuning (PEFT) optimizes the fine-tuning process for large language models (LLMs) by making slight adjustments to a model's weights, significantly reducing computational costs compared to full fine-tuning, where every weight is adjusted. The approach involves starting with a pretrained model that already possesses extensive knowledge, then applying task-specific datasets for enhancement.PEFT employs three main strategies:1. **Selective**: Fine-tunes only a subset of the model's parameters, reducing the computational burden.2. **Reparameterization**: Utilizes low-rank representations to adjust model weights. An example is LoRA (Low Rank Adaptation), where weight matrices are decomposed into smaller, low-rank matrices, allowing for a significant reduction in the number of parameters that need to be trained while maintaining performance.3. **Additive**: Represents another method within PEFT for fine-tuning.Overall, PEFT aims to streamline the fine-tuning process, making it more efficient while minimizing the changes required to the pretrained model's weights.\n",
            "Sources:\n",
            "\tNode ID\t 6be88fa3-2f8b-43e7-aba0-d874b39809fc\n",
            "\tText\t # FourierFT: Discrete Fourier Transformation Fine-Tuning[FourierFT](https://huggingface.co/papers/2405.03003) is a parameter-efficient fine-tuning technique that leverages Discrete Fourier Transform to compress the model's tunable weights. This method outperforms LoRA in the GLUE benchmark and common ViT classification tasks using much less parameters.FourierFT currently has the following constraints:- Only `nn.Linear` layers are supported.- Quantized layers are not supported.If these constraints don't work for your use case, consider other methods instead.The abstract from the paper is:> Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices A and B to represent the weight change, i.e., Delta W=BA. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats Delta W as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover Delta W. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M.## FourierFTConfig[[autodoc]] tuners.fourierft.config.FourierFTConfig## FourierFTModel[[autodoc]] tuners.fourierft.model.FourierFTModel\n",
            "\tScore\t 0.42088463893369404\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 2b4590b8-4c9d-4d51-a81f-9a13979593a8\n",
            "\tText\t SAMSUM is one of the datasets that FLAN T5 uses. There are several pre-trained FLAN T5 models that have been fine-tuned on SAMSUM  including Phil Schmid/flan-t5-base-samsum and jasonmcaffee/flan-t5-large-samsum on Hugging Face. If we want to fine-tune the FLAN T5 model specifically for formal dialogue conversations  we can do so using the DIALOGUESUM dataset.   Models fine-tuned on DialogSum can be applied to areas like customer support  meeting minutes generation  chatbot summarization  and more.   2. PEFT (Parameter efficient fine tuning)Training LLMs is computationally intensive. Full finetuning is computationally expensive as it might change each weight in the model. First  we start with a pretrained LLM like GPT-3. This model already has a vast amount of knowledge and understanding of language. Then we provide task-specific datasets  which could be data for question answering or sentiment analysis or any other customer dataset. During training  full finetuning process makes slight adjustments to every weight in the pretrained model. While the model weights are substantial  we have other important aspects during training like Optimizer  which adds up to the cost. For example  Optimizer States  gradients  forward activation  and temporary memory. These additional components add up to the training cost.   Three main approaches are used in PEFT: Selective / reparameterization/additive.   1. SelectiveHere  we select a subset of initial LLM parameters to fine-tune.   2. ReparameterizationWe reparameterize model weights using a low-rank representation. We will discuss LoRA in detail below.   LORA: Low Rank Representation:   Each layer in a transformer architecture has multiple weight matrices for different operations  like self-attention or feed-forward networks. These matrices can have different sizes depending on the specific layer and configuration. Let us take an example by picking a matrix of size 512 x 64 = 32 768 parameters. Let us now see LoRA with rank = 8.   Original Weight Matrix: Dimensions: 512 x 64  Parameters: 32 768 (512 x 64)Matrix A (Rank Decomposition):\n",
            "\tScore\t 0.3833038456089033\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# First Question:\n",
        "response = chat_engine.chat(\"Use the tool to answer, how does parameter efficient finetuning work?\")\n",
        "\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3RRmiJEQ5R1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096db98a-1b26-45b4-e09d-a5ab3d649c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Sure! Here's a joke for you:Why did the scarecrow win an award?Because he was outstanding in his field!\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Second Question:\n",
        "response = chat_engine.chat(\"Could you tell me a joke?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8eOzp5Xc5Vbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762239c4-acb3-448e-892e-4b89838add52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t The first question you asked was, \"How does parameter efficient finetuning work?\"\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Third Question: (check if it can recall previous interactions)\n",
        "response = chat_engine.chat(\"What was the first question I asked?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7jfiLpru5VZT"
      },
      "outputs": [],
      "source": [
        "# Reset the session to clear the memory\n",
        "chat_engine.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Jt0q8RW25VXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c66e54b-7053-433e-a5fc-79d7463d9191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t The first question you asked was, \"What was the first question I asked?\"\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Fourth Question: (don't recall the previous interactions.)\n",
        "response = chat_engine.chat(\"What was the first question I asked?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Egsib7yPJGR"
      },
      "source": [
        "# Streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zanJeMbaPJcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651a09b9-5e0c-4ad0-dbb4-ceeffdd7a053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval-Augmented Generation (RAG) and Parameter-Efficient Fine-Tuning (PEFT) are two techniques used in the realm of machine learning, particularly in natural language processing. RAG combines two primary components: retrieval and generation. It first retrieves relevant information from external sources through an indexing and searching process, leveraging either sparse or dense retrieval methods. After obtaining the necessary context, the generation component produces coherent and contextually relevant responses, integrating the retrieved information without requiring extensive fine-tuning. In contrast, PEFT allows for fine-tuning large pretrained models by only adjusting a small subset of additional parameters, rather than the entire model. This parameter-efficient approach drastically reduces computational and storage costs while still delivering performance comparable to fully fine-tuned models. The main difference lies in their approach: RAG focuses on enhancing generative capabilities through external information retrieval, while PEFT streamlines adapting large models to specific tasks with minimal changes to their parameters."
          ]
        }
      ],
      "source": [
        "# Stream the words as soon as they are available instead of waiting for the model to finish generation.\n",
        "streaming_response = chat_engine.stream_chat(\n",
        "    \"Write a paragraph explaining how RAG and PEFT work, and highlight the differences between them.\"\n",
        ")\n",
        "streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuRgOJ2AHMJh"
      },
      "source": [
        "## Condense Question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb2Lt41jq145"
      },
      "source": [
        "Enhance the input prompt by looking at the previous chat history along with the present question. The refined prompt can then be used to fetch the nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v0gmM5LGIaRl"
      },
      "outputs": [],
      "source": [
        "# Define GPT-4 model that will be used by the chat_engine to improve the query.\n",
        "gpt4 = OpenAI(temperature=0.9, model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EDWsaBTBIhK7"
      },
      "outputs": [],
      "source": [
        "chat_engine = vector_index.as_chat_engine(\n",
        "    chat_mode=\"condense_question\", llm=gpt4, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h4c--hJ75VU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8de371-91e8-40c0-b785-118691f4e83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\n",
            "Response:\n",
            "\t Retrieval-Augmented Generation (RAG) works by combining pretraining and retrieval-based models to enhance the performance of generative models. It involves several key processing steps, including query classification, retrieval of relevant documents, reranking these documents based on their relevance, repacking them into a structured format, and summarizing the key information for response generation. This approach solves the problem of generative large language models producing outdated information or fabricating facts by integrating up-to-date and relevant content from external knowledge sources, thereby improving the accuracy and reliability of the generated information.\n",
            "Sources:\n",
            "\tNode ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
            "\tText\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
            "\tScore\t 0.5942003378877615\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
            "\tText\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
            "\tScore\t 0.5742996423371074\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\n",
        "    \"How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\"\n",
        ")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysL9ONePOsGB"
      },
      "source": [
        "## ReAct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiEFmxAtrmF-"
      },
      "source": [
        "ReAct is an agent-based chat mode that uses a loop to decide on querying a data engine during interactions, offering flexibility but relying on the Large Language Model's quality for effective responses, requiring careful management to avoid inaccurate answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-M1jWoKXOs2t"
      },
      "outputs": [],
      "source": [
        "chat_engine = vector_index.as_chat_engine(chat_mode=\"react\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UZkEW1SSOs0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bce2dd-b442-4131-a347-3eceadc17efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Which company developed Claude 3.5 Sonnet, and what is its primary application?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"}\n",
            "Got output: Claude 3.5 Sonnet was developed by Anthropic. Its primary application is to serve as a free-tier model that balances cost and features, making it suitable for tasks like creative writing and answering questions.\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\n",
        "    \"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eW5P1lD4Osxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c74c417-c6e1-4184-e7a1-9925420015d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Claude 3.5 Sonnet was developed by Anthropic. Its primary application is to serve as a free-tier model that balances cost and features, making it suitable for tasks like creative writing and answering questions.\n",
            "Sources:\n",
            "\tNode ID\t 4558b7d3-7f77-4f55-b0e7-64d385820117\n",
            "\tText\t 2. Dedicated to safety and security   It is a well-known fact that Anthropic prioritizes responsible AI development the most  and it is clearly seen in Claudes design. This generative AI model is trained on a carefully curated dataset thus it minimizes biases and factual errors to a large extent. On top of that  Claude also undergoes rigorous safety checks to prevent the generation of harmful and misleading content.   3. Emphasizes Explainability   While many of the AI and LLMs currently operate as black boxes  Claude offers a high level of explainability surpassing other models. This means it can explain the reasoning and decision-making process behind all of its responses. Therefore  it helps users to use this model confidently and they can be assured about the credibility of the information provided.   Claude FamilyClaude AI comes in a family of 3 generative AI models. Users can choose from these three models based on their power requirements and budget.   1. Haiku: It is the most budget-friendly option and offers fast response times. This can be perfect for simple tasks that require short context. This is yet to be launched but users can expect it to be highly cost-effective and cheaper as compared to other models.   2. Sonnet: This is a free-tier model and serves as an excellent starting point by offering a balance between cost and features. It can effectively handle tasks like writing different creative text formats and answering questions  just like Open AIs ChatGPT.   3. Opus: This is the most powerful generative AI model by Claude AI; however  users require a premium subscription to use this AI Chatbot. It can perform complex tasks easily that require a large context window. So  if you are looking for a generative AI that can do research  summarize lengthy documents  or help with consistent lengthy conversations  then this model will be the best option.   ChatGPT vs. Claude AI: How do they differ?Claude AI and OpenAIs ChatGPT both are very powerful LLM models. But they are designed for various purposes. Lets compare.   Strengths:    Claude: It is great in performing tasks requiring long-term context as discussed above. They can maintain consistency throughout their response in extended conversations. Also  their explainability\n",
            "\tScore\t 0.3264445185304059\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 0e97549b-bf5a-4e4f-9f5a-d6a478126874\n",
            "\tText\t Claude AI and ChatGPT are both powerful and popular generative AI models revolutionizing various aspects of our lives. Here  let us learn more about Claude AI and its benefits   Ever since the launch of ChatGPT  many other companies have also joined the race to bring excellent generative AI models into the world that not only help users create realistic content but are also safe to use  and free from bias. While Open AIs ChatGPT and Googles Bard  now Gemini  get most of the limelight  Claude AI stands out for its impressive features and being the most reliable and ethical Large Language Model.   In this article  we will learn more about what Claude AI is and what are its unique features. We will also discuss how it differs from the most popular generative AI tool ChatGPT.   Claude AI is developed by Anthropic  an AI startup company backed by Google and Amazon  and is dedicated to developing safe and beneficial AI. Claude AI is an LLM based on the powerful transformer architecture and like OpenAIs ChatGPT  it can generate text  translate languages  as well as write different kinds of compelling content. It can interact with users like a normal AI chatbot; however  it also boasts some unique features that make it different from others.   1. Larger Context Window   One of the Claude AIs biggest capabilities is that it can process huge chunks of text as compared to ChatGPT. While ChatGPT struggles to process and keep track of information in long conversations  Claudes context window is huge (spanning up to 150 pages)  which helps users to do more coherent and consistent conversations  especially when it comes to long documents.   2. Dedicated to safety and security   It is a well-known fact that Anthropic prioritizes responsible AI development the most  and it is clearly seen in Claudes design. This generative AI model is trained on a carefully curated dataset thus it minimizes biases and factual errors to a large extent. On top of that  Claude also undergoes rigorous safety checks to prevent the generation of harmful and misleading content.   3. Emphasizes Explainability   While many of the AI and LLMs currently operate as black boxes  Claude offers a high level of explainability surpassing other models. This means it\n",
            "\tScore\t 0.29960111331300543\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zf6r2AmFOsca"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llamaindexkernel",
      "language": "python",
      "name": "llamaindexkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bbc63358f02540f6a5e476c4d02fd964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48dcb706fd614fc8943e15d15a0b0fe2",
              "IPY_MODEL_bc5a1a1acdbc4afb95c8e13ebed9b495",
              "IPY_MODEL_41e89cbfd4804323989193fcb3dd7f7b"
            ],
            "layout": "IPY_MODEL_16e1b25eb22a4e0a896afb9f85274ece"
          }
        },
        "48dcb706fd614fc8943e15d15a0b0fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03c5b98d4ea94568ad110f4e4bd402c6",
            "placeholder": "​",
            "style": "IPY_MODEL_9fcb1f1b5a324d539bdf9f36df413d76",
            "value": "vectorstore.zip: 100%"
          }
        },
        "bc5a1a1acdbc4afb95c8e13ebed9b495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5408c84b29a74670bd8d03f2d538926c",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dec4652142e442658c69d473abcc7b72",
            "value": 97198458
          }
        },
        "41e89cbfd4804323989193fcb3dd7f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e16e13c258d4744b804cb1e0274f1c1",
            "placeholder": "​",
            "style": "IPY_MODEL_86b5a4d6f70043d3a65de02b840e8937",
            "value": " 97.2M/97.2M [00:02&lt;00:00, 42.8MB/s]"
          }
        },
        "16e1b25eb22a4e0a896afb9f85274ece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c5b98d4ea94568ad110f4e4bd402c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fcb1f1b5a324d539bdf9f36df413d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5408c84b29a74670bd8d03f2d538926c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec4652142e442658c69d473abcc7b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e16e13c258d4744b804cb1e0274f1c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b5a4d6f70043d3a65de02b840e8937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}